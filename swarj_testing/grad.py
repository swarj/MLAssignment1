# -*- coding: utf-8 -*-
"""grad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxpnKDEc5emiXGFDD6lFel4Qb8zXE3oJ
"""

#Gradient Descent Fn
# yhat = wx + b
# loss = (y-yhat)^2 / N
import numpy as np

x = np.random.randn(10,1)
y = 5*x + np.random.rand()
#Parameter
w = 0.0
b = 0.0
#Hyperparam
learning_rate = 0.01

from IPython.core.display import display_markdown
def descend(x,y,w,b,learning_rate):
  dldw = 0.0
  dldb = 0.0
  N = x.shape[0] #number of values (10)
  # loss = (y-(wx+b)))^2
  for xi, yi in zip(x,y):
    #dldw = 2(0-(x))* -x
    dldw += -2*xi*(yi-(w*xi+b))
    dldb += -2*(yi-(w*xi+b))
  #update W and B parameter
  w = w - learning_rate*(1/N)*dldw
  b = b - learning_rate*(1/N)*dldb
  return w, b

for epoch in range(400): #number of updates, more will get us closer to Y
  w,b = descend(x,y,w,b,learning_rate)
  yhat = w*x + b
  loss = np.divide(np.sum((y-yhat)**2, axis = 0), x.shape[0]) #using NumPy to sum all the losses, can be done without numpy # divides by number of rows or (N)
  print(f'{epoch} loss is {loss}, parameters w: {w}, b: {b}')