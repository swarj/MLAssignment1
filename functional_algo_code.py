# -*- coding: utf-8 -*-
"""functional_algo_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rO7iptXiasOJ-fGazwxcBVrZZSejwR-h
"""

import pandas as pd
import numpy as np

url = "https://raw.githubusercontent.com/marcoan01/MLA/main/ale.csv"
df = pd.read_csv(url)

# Features
x1 = df['anchor_ratio']
x2 = df['trans_range']
x3 = df['node_density']
y = df['ale'] #output var

#normalized
x1 = (x1 - x1.mean()) / x1.std()
x2 = (x2 - x2.mean()) / x2.std()
x3 = (x3 - x3.mean()) / x3.std()

# add m0 variable to matrix
x = np.c_[x1,x2,x3, np.ones(x1.shape[0])]

print_y = np.array(y)

#variables
lrate = 0.01 #step size
epoch = 2000 #iterations
np.random.seed(501) #random starter values for slopes
coefficient = np.random.rand(4) # 3 params + 1s
print("Initial values of coefficients: ", coefficient)
n = y.size # number of examples in the dataset


#Gradient Descent Implementation
def grad_desc(x,y,coefficient, epoch, lrate):
    #initialize arrays
    prev_coefficient = [coefficient]
    prev_cost = []
    for i in range(epoch):
      pred = np.dot(x, coefficient) #w0 + w1x1+w2x2 + w3x3
      #print(pred)
      deviation = pred - y #how far the prediction is off the line from the actual
      cost = 1/(2*n) * np.dot(deviation.T, deviation) #cost function 1(2n) * MSE
      prev_cost.append(cost)  #update costs based on new cost amount
      derivative = (1/n) * lrate * np.dot(x.T, deviation) #derivative equation to change coefficients, dotproduct does summation
      coefficient = coefficient - derivative #adjust coefficient
      #for j in range(107):
        #print("Prediction: ", pred[j])
        #print("Actual: ", y[j])
      prev_coefficient.append(coefficient) #update coeffs save previous ones
      #print output to compare results at the end
      if i == 1999:
        print("PREDICTIONS vs ACTUAL")
        print("PREDICTION SET: ", pred)
        print("ACTUAL SET: ", print_y)
    return prev_coefficient, prev_cost

#function call
prev_coefficient, prev_cost = grad_desc(x,y,coefficient,epoch, lrate)

#print final values
print("Final Coefficients: ",prev_coefficient[-1])
print("Final Cost: ", prev_cost[-1] )